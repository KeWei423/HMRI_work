{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project NaCl v. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File name parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_notes(name):\n",
    "    \"\"\"\n",
    "    extract notes inside the parentheses before filtering and parsing\n",
    "    --------------------------------------------------------\n",
    "    input:\n",
    "        sequence_name : string\n",
    "            sequence with notes within parentheses\n",
    "        \n",
    "    return:\n",
    "        sequence_name : string\n",
    "            sequence info without the parentheses and notes inside of them\n",
    "        notes : string\n",
    "            the notes that were in the parentheses\n",
    "    \"\"\"\n",
    "    op = None\n",
    "    ed = None\n",
    "    notes = None\n",
    "    \n",
    "    for i in range(len(name)):\n",
    "        if name[i] == '(':\n",
    "            op = i\n",
    "        if name[i] == ')':\n",
    "            ed = i\n",
    "            break\n",
    "    \n",
    "    if ed:\n",
    "        notes = name[op:ed+1]\n",
    "        notes = notes.replace('_', '-')\n",
    "        name = name[:op-1] + '_' + name[ed+2:]\n",
    "    \n",
    "    return name, notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(sequence_name):\n",
    "    \"\"\"\n",
    "    Convert the input from string to list \n",
    "    --------------------------------------------------------\n",
    "    input:\n",
    "        sequence_name: string\n",
    "        \n",
    "    return:\n",
    "        str_list : list\n",
    "            splited the string into items for parsing later\n",
    "    \"\"\"\n",
    "    str_list = sequence_name.replace('-','_').replace('.','_').split('_')\n",
    "#     if not (str_list[-1] == 'NII' or str_list[-1] == 'JSON'):\n",
    "#         return None\n",
    "    \n",
    "    str_list = list(filter(None, str_list)) \n",
    "    return str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(info, notes=None):\n",
    "    \"\"\"\n",
    "    Convert the input from list to string with notes added if needed be\n",
    "    --------------------------------------------------------\n",
    "    input:\n",
    "        sequence_info: array-like\n",
    "            splited the string \n",
    "        notes : string or None\n",
    "            notes to be added into the file name\n",
    "        \n",
    "    return:\n",
    "        name : string \n",
    "            sequence_info joined together with notes added if need be\n",
    "    \"\"\"\n",
    "    if notes:\n",
    "        info.insert(-2, notes)\n",
    "        name = ('_'.join(info[:-1]))+'.'+info[-1].lower()\n",
    "    else:\n",
    "        name = ('_'.join(info[:-1]))+'.'+info[-1].lower()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_standard(name):\n",
    "    \"\"\"\n",
    "    clean up the user input sequence name into a standardized naming convetion for sorting later\n",
    "             MEN --> MN\n",
    "        WHITTIER --> WH\n",
    "        SAGITTLE --> SAG\n",
    "           AXIAL --> AX\n",
    "         CORONAL --> COR\n",
    "         OBLIQUE --> OBL\n",
    "              RT --> RIGHT\n",
    "              LT --> LEFT\n",
    "    --------------------------------------------------------\n",
    "    input:\n",
    "        name: string\n",
    "            name to be standardized\n",
    "    \n",
    "    return:\n",
    "        n_name : string\n",
    "            standardized file name\n",
    "    \"\"\"\n",
    "    n_name = name.replace('MEN', 'MN').replace('WHITTIER', 'WH')\n",
    "    n_name = n_name.replace('SAGITTAL', 'SAG').replace('AXIAL', 'AX').replace('CORONAL', 'COR').replace('OBLIQUE', 'OBL')\n",
    "    n_name = n_name.replace('RT', 'RIGHT').replace('LT', 'LEFT')\n",
    "    \n",
    "    info = get_info(n_name)\n",
    "    \n",
    "    if not info[-2][-1].isdigit():\n",
    "        trash.append(name)\n",
    "        return None\n",
    "    \n",
    "#     print(info)\n",
    "    \n",
    "    if 'RE' in info:\n",
    "        info.remove('RE')\n",
    "        info.insert(-2, '(RE)')\n",
    "    \n",
    "    ID = info[0]\n",
    "    # ID does not follow designation [2-3 letter cohort designation][3-4 number designation]\n",
    "    if ID.isdigit():\n",
    "        info[0] = 'WH'+info[0]\n",
    "    \n",
    "    n_name = '_'.join(info)\n",
    "    \n",
    "    return n_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_FSPGR(name, trash):\n",
    "    \"\"\"\n",
    "    Renames the file passed in\n",
    "    ---------------------------------------------------------------\n",
    "    input:\n",
    "        name : str\n",
    "            name of file to be renamed\n",
    "        trash : list\n",
    "            if file is filtered out, it's thrown into this list\n",
    "    output:\n",
    "        n_name : str\n",
    "            name of file after being renamed\n",
    "    \"\"\"\n",
    "    n_name, notes = extract_notes(name)\n",
    "    \n",
    "    info = get_info(n_name)\n",
    "    \n",
    "    if any(word in info for word in ['T1', 'BRAVO']):\n",
    "        trash.append(name)\n",
    "        return None\n",
    "    \n",
    "    n_name = get_name(info, notes)\n",
    "    \n",
    "    return n_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_CUBE(name, trash):\n",
    "    \"\"\"\n",
    "    Renames the file passed in\n",
    "    ---------------------------------------------------------------\n",
    "    input:\n",
    "        name : str\n",
    "            name of file to be renamed\n",
    "        trash : list\n",
    "            if file is filtered out, it's thrown into this list\n",
    "    output:\n",
    "        n_name : str\n",
    "            name of file after being renamed\n",
    "    \"\"\"\n",
    "    n_name, notes = extract_notes(name)\n",
    "    \n",
    "    info = get_info(n_name)\n",
    "    \n",
    "    if any(word in info for word in ['3D', 'BRAVO']):\n",
    "        trash.append(name)\n",
    "        return None\n",
    "    \n",
    "    n_name = get_name(info, notes)\n",
    "    \n",
    "    return n_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_minimal(name, trash):\n",
    "    \"\"\"\n",
    "    Renames the file passed in\n",
    "    ---------------------------------------------------------------\n",
    "    input:\n",
    "        name : str\n",
    "            name of file to be renamed\n",
    "        trash : list\n",
    "            if file is filtered out, it's thrown into this list\n",
    "    output:\n",
    "        n_name : str\n",
    "            name of file after being renamed\n",
    "    \"\"\"\n",
    "    n_name, notes = extract_notes(name)\n",
    "    \n",
    "    info = get_info(n_name)\n",
    "    \n",
    "    n_name = get_name(info, notes)\n",
    "    \n",
    "    return n_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_diffusion(name, trash):\n",
    "    \"\"\"\n",
    "    Renames the file passed in\n",
    "    ---------------------------------------------------------------\n",
    "    input:\n",
    "        name : str\n",
    "            name of file to be renamed\n",
    "        trash : list\n",
    "            if file is filtered out, it's thrown into this list\n",
    "    output:\n",
    "        n_name : str\n",
    "            name of file after being renamed\n",
    "    \"\"\"\n",
    "    name = name.replace('30_DIRECTION', '(30_DIRECTIONS)').replace('ASSET', '(ASSET)')\n",
    "    n_name, notes = extract_notes(name)\n",
    "    if notes and notes[-2] != 'S':\n",
    "        notes = notes[:-1] + 'S)'\n",
    "    \n",
    "    info = get_info(n_name)\n",
    "    if 'AX' not in info:\n",
    "        info.insert(1, 'AX')\n",
    "        \n",
    "    if any(word in info for word in ['MAP', 'MAPS', 'MULEFTI']):\n",
    "        trash.append(name)\n",
    "        return None\n",
    "    \n",
    "    n_name = get_name(info, notes)\n",
    "    \n",
    "    return n_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_ASL(name, trash):\n",
    "    \"\"\"\n",
    "    Renames the file passed in\n",
    "    ---------------------------------------------------------------\n",
    "    input:\n",
    "        name : str\n",
    "            name of file to be renamed\n",
    "        trash : list\n",
    "            if file is filtered out, it's thrown into this list\n",
    "    output:\n",
    "        n_name : str\n",
    "            name of file after being renamed\n",
    "    \"\"\"\n",
    "    n_name, notes = extract_notes(name)\n",
    "    \n",
    "    info = get_info(n_name)\n",
    "    if 'AX' not in info:\n",
    "        info.insert(1, 'AX')\n",
    "        \n",
    "    if any(word in info for word in ['3D', '2025', '2525']):\n",
    "        trash.append(name)\n",
    "        return None\n",
    "    \n",
    "    n_name = get_name(info, notes)\n",
    "    \n",
    "    return n_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_SWAN(name, trash):\n",
    "    \"\"\"\n",
    "    Renames the file passed in\n",
    "    ---------------------------------------------------------------\n",
    "    input:\n",
    "        name : str\n",
    "            name of file to be renamed\n",
    "        trash : list\n",
    "            if file is filtered out, it's thrown into this list\n",
    "    output:\n",
    "        n_name : str\n",
    "            name of file after being renamed\n",
    "    \"\"\"\n",
    "    n_name, notes = extract_notes(name)\n",
    "    \n",
    "    info = get_info(n_name)\n",
    "        \n",
    "    if any(word in info for word in ['3D', 'MIDBRAIN']):\n",
    "        trash.append(name)\n",
    "        return None\n",
    "    \n",
    "    n_name = get_name(info, notes)\n",
    "    \n",
    "    return n_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_CSF(name, trash, c2, aq):\n",
    "    \"\"\"\n",
    "    Renames the file passed in\n",
    "    ---------------------------------------------------------------\n",
    "    input:\n",
    "        name : str\n",
    "            name of file to be renamed\n",
    "        trash : list\n",
    "            if file is filtered out, it's thrown into this list\n",
    "        c2 : list\n",
    "            if file is a c2 scan, its thrown into this list\n",
    "        aq: list\n",
    "            if file is a aquaduct scan, its thrown into this list\n",
    "    output:\n",
    "        n_name : str\n",
    "            name of file after being renamed and is unable to be identified if its c2 or aq\n",
    "    \"\"\"\n",
    "    info = get_info(name)\n",
    "        \n",
    "    if any(word in info for word in ['5T', 'DEEP', 'CF', 'V']):\n",
    "        trash.append(name)\n",
    "        return None\n",
    "    \n",
    "    if 'VENC' not in info:\n",
    "        trash.append(name)\n",
    "        return None\n",
    "    \n",
    "    index = info.index('VENC')\n",
    "    if not info[index+1].isdigit():\n",
    "        trash.append(name)\n",
    "        return None\n",
    "    \n",
    "    info[index] = '('+info[index]\n",
    "    info[index+1] = info[index+1]+')'\n",
    "    \n",
    "    n_name = get_name(info)\n",
    "    n_name, notes = extract_notes(n_name)\n",
    "    n_name = get_name(get_info(n_name), notes).replace('CSF_FLOW', 'PC').replace('CSF', 'PC')\n",
    "    \n",
    "    if 'C2' in info:\n",
    "        c2.append(n_name)\n",
    "        return None\n",
    "    \n",
    "    if 'AQ' in info:\n",
    "        aq.append(n_name)\n",
    "        return None\n",
    "    \n",
    "    return n_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_blood(name, trash):\n",
    "    \"\"\"\n",
    "    Renames the file passed in\n",
    "    ---------------------------------------------------------------\n",
    "    input:\n",
    "        name : str\n",
    "            name of file to be renamed\n",
    "        trash : list\n",
    "            if file is filtered out, it's thrown into this list\n",
    "    output:\n",
    "        n_name : str\n",
    "            name of file after being renamed\n",
    "    \"\"\"\n",
    "    info = get_info(name)\n",
    "        \n",
    "    if any(word in info for word in ['MRA']):\n",
    "        trash.append(name)\n",
    "        return None\n",
    "    \n",
    "    if 'VENC' not in info:\n",
    "        trash.append(name)\n",
    "        return None\n",
    "    \n",
    "    index = info.index('VENC')\n",
    "    if not info[index+1].isdigit():\n",
    "        trash.append(name)\n",
    "        return None\n",
    "    \n",
    "    info[index] = '('+info[index]\n",
    "    info[index+1] = info[index+1]+')'\n",
    "    \n",
    "    n_name = get_name(info)\n",
    "    n_name, notes = extract_notes(n_name)\n",
    "    n_name = get_name(get_info(n_name), notes)\n",
    "    \n",
    "    return n_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "root = '/media/ke/8tb_part2/FSL_work/all_info'\n",
    "\n",
    "\n",
    "accept = []\n",
    "bad_json = []\n",
    "reject = []\n",
    "\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        name, extension = os.path.splitext(file)\n",
    "        if extension == '.json':\n",
    "            try:\n",
    "                with open(path + '/' + file) as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                    series_num = data['SeriesNumber']\n",
    "                    if series_num < 100 and series_num > 2:\n",
    "                        if fnmatch.fnmatch(file, '*FLAIR*') or fnmatch.fnmatch(file, '*T2*') or fnmatch.fnmatch(file, '*FSGPR*'):\n",
    "                            mr_aq_type = data['MRAcquisitionType']\n",
    "                            if mr_aq_type == '2D':\n",
    "                                reject.append(name)\n",
    "                            else:\n",
    "                                accept.append(name)\n",
    "                        else:\n",
    "                            accept.append(name)\n",
    "                    else:\n",
    "                        reject.append(name)\n",
    "            except:\n",
    "                bad_json.append(name)\n",
    "#                 print(file)\n",
    "\n",
    "fspgr=[]\n",
    "flair = []\n",
    "t2 = []\n",
    "bold = []\n",
    "dwi=[]\n",
    "dti = []\n",
    "dki = []\n",
    "swan = []\n",
    "asl=[]\n",
    "candy_cane=[]\n",
    "arch=[]\n",
    "c2 = []\n",
    "aq = []\n",
    "csf_other = []\n",
    "carotid = []\n",
    "probe = [] # MRS\n",
    "\n",
    "trash = []\n",
    "\n",
    "c2_len = 0\n",
    "aq_len = 0\n",
    "    \n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        name, extension = os.path.splitext(file)\n",
    "        if name in accept:\n",
    "            file_upper=base_standard(file.upper())\n",
    "            if not file_upper:\n",
    "                trash.append(file)\n",
    "                continue\n",
    "            if fnmatch.fnmatch(file_upper, '*MPR*'):    \n",
    "                pass\n",
    "            if fnmatch.fnmatch(file_upper, '*FSPGR*'):    \n",
    "                # 17 SAG_T1_3D\n",
    "#                 print(\"T1:\", file_upper)\n",
    "                n_name = filt_FSPGR(file_upper, trash)\n",
    "                if n_name is not None:\n",
    "#                     print('FSPGR:', n_name)\n",
    "                    fspgr.append((file, n_name))\n",
    "                pass\n",
    "            elif fnmatch.fnmatch(file_upper, '*FLAIR*'): \n",
    "#                 print(\"FLAIR:\", file_upper)\n",
    "                n_name = filt_CUBE(file_upper, trash)\n",
    "                if n_name is not None:\n",
    "#                     print('FLAIR:',n_name)\n",
    "                    flair.append((file, n_name))\n",
    "                pass\n",
    "            elif fnmatch.fnmatch(file_upper, '*T2*'): \n",
    "#                 print(\"T2:\", file_upper)\n",
    "                n_name = filt_CUBE(file_upper, trash)\n",
    "                if n_name is not None:\n",
    "#                     print('T2:',n_name)\n",
    "                    t2.append((file, n_name))\n",
    "                pass\n",
    "            elif fnmatch.fnmatch(file_upper, '*BOLD*'):\n",
    "#                 print('BOLD:', file_upper)\n",
    "                n_name = filt_minimal(file_upper, trash)\n",
    "                if n_name is not None:\n",
    "#                     print('BOLD:',n_name)\n",
    "                    bold.append((file, n_name))\n",
    "                pass\n",
    "            elif fnmatch.fnmatch(file_upper, '*DWI*'):\n",
    "#                 print('DWI:', file_upper)\n",
    "                n_name = filt_diffusion(file_upper, trash)\n",
    "                if n_name is not None:\n",
    "#                     print('DWI:',n_name)\n",
    "                    dwi.append((file, n_name))\n",
    "                pass\n",
    "            elif fnmatch.fnmatch(file_upper, '*DTI*'):\n",
    "#                 print('DTI:', file_upper)\n",
    "                n_name = filt_diffusion(file_upper, trash)\n",
    "                if n_name is not None:\n",
    "#                     print('DTI:',n_name)\n",
    "                    dti.append((file, n_name))\n",
    "                pass\n",
    "            elif fnmatch.fnmatch(file_upper, '*DKI*'):\n",
    "#                 print('DKI:', file_upper)\n",
    "                n_name = filt_diffusion(file_upper, trash)\n",
    "                if n_name is not None:\n",
    "#                     print('DKI:',n_name)\n",
    "                    dki.append((file, n_name))\n",
    "                pass\n",
    "            elif fnmatch.fnmatch(file_upper, '*ASL*'):\n",
    "#                 print('ASL:', file_upper)\n",
    "                n_name = filt_ASL(file_upper, trash)\n",
    "                if n_name is not None:\n",
    "#                     print('ASL:',n_name)\n",
    "                    asl.append((file, n_name))\n",
    "                pass\n",
    "            elif fnmatch.fnmatch(file_upper, '*SWAN*'):\n",
    "#                 print('SWAN      :', file_upper)\n",
    "                n_name = filt_SWAN(file_upper, trash)\n",
    "                if n_name is not None:\n",
    "#                     print('SWAN:',n_name)\n",
    "                    asl.append((file, n_name))\n",
    "                pass\n",
    "            elif fnmatch.fnmatch(file_upper, '*CSF*'):\n",
    "#                 print('CSF       :', file_upper)\n",
    "                n_name = filt_CSF(file_upper, trash, c2, aq)\n",
    "#                 if len(c2) != c2_len:\n",
    "#                     c2_len += 1\n",
    "#                     print('\\tC2 :', c2[-1])\n",
    "#                 if len(aq) != aq_len:\n",
    "#                     aq_len += 1\n",
    "#                     print('\\tAQ :', aq[-1])\n",
    "                if n_name is not None:\n",
    "#                     print('CSF:',n_name)\n",
    "                    csf_other.append(n_name)\n",
    "                pass\n",
    "            elif fnmatch.fnmatch(file_upper, '*CAROTID*'):\n",
    "#                 print('CAROTID   :', file_upper)\n",
    "                n_name = filt_blood(file_upper, trash)\n",
    "                if n_name is not None:\n",
    "#                     print('CAROTID:',n_name)\n",
    "                    asl.append((file, n_name))\n",
    "                pass\n",
    "            elif 'LOC' not in file_upper and fnmatch.fnmatch(file_upper, '*CANDY_CANE*'):\n",
    "#                 print('CANDY_CANE:', file_upper)\n",
    "                n_name = filt_minimal(file_upper, trash)\n",
    "                if n_name is not None:\n",
    "#                     print('CANDY_CANE:',n_name)\n",
    "                    bold.append((file, n_name))\n",
    "                pass\n",
    "            elif 'LOC' not in file_upper and fnmatch.fnmatch(file_upper, '*ARCH*'):\n",
    "#                 print('ARCH      :', file_upper)\n",
    "                n_name = filt_blood(file_upper, trash)\n",
    "                if n_name is not None:\n",
    "#                     print('ARCH:',n_name)\n",
    "                    asl.append((file, n_name))\n",
    "                pass\n",
    "            elif fnmatch.fnmatch(file_upper, '*PROBE*'):\n",
    "#                 print('PROBE     :', file_upper)\n",
    "                asl.append(file)\n",
    "                pass\n",
    "            else:\n",
    "#                 print('\\t', file_upper)\n",
    "                pass\n",
    "\n",
    "filtered = {'T1' : fspgr,\n",
    "            'FLAIR' : flair,\n",
    "            'T2' : t2,\n",
    "            'BOLD' : bold,\n",
    "            'DWI' : dwi,\n",
    "            'DTI' : dti,\n",
    "            'DKI' : dki,\n",
    "            'SWAN' : swan,\n",
    "            'ASL' : asl,\n",
    "            'CANDY CANE' : candy_cane,\n",
    "            'ARCH' : arch,\n",
    "            'C2' : c2,\n",
    "            'AQ' : aq,\n",
    "            'CSF_OTHER' : csf_other,\n",
    "            'CAROTID' : carotid,\n",
    "            'PROBE': probe}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming the CVR Gas Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ke/Desktop/all_gas/'\n",
    "nifti_dir = '/media/ke/8tb_part2/FSL_work/all_info/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>EndTidal_Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>BR001</td>\n",
       "      <td>20170511</td>\n",
       "      <td>/home/ke/Desktop/all_gas/BR_001_20170511_EDITS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>BR006</td>\n",
       "      <td>20171107</td>\n",
       "      <td>/home/ke/Desktop/all_gas/BR_006_20171107_EDITS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>BR011</td>\n",
       "      <td>20190226</td>\n",
       "      <td>/home/ke/Desktop/all_gas/BR_011_20190226_edits...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>BR011</td>\n",
       "      <td>20171101</td>\n",
       "      <td>/home/ke/Desktop/all_gas/BR_011_20171101_EDITS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>BR012</td>\n",
       "      <td>20171108</td>\n",
       "      <td>/home/ke/Desktop/all_gas/BR_012_20171108_EDITS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>WH1707</td>\n",
       "      <td>20190401</td>\n",
       "      <td>/home/ke/Desktop/all_gas/WH_1707_20190401_edit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>WH1716</td>\n",
       "      <td>20190213</td>\n",
       "      <td>/home/ke/Desktop/all_gas/WH_1716_20190213_edit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>WH1721</td>\n",
       "      <td>20190813</td>\n",
       "      <td>/home/ke/Desktop/all_gas/WH_1721_20190813_EDIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>WH1726</td>\n",
       "      <td>20190702</td>\n",
       "      <td>/home/ke/Desktop/all_gas/WH_1726_20190702_EDIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>WH5002</td>\n",
       "      <td>20180316</td>\n",
       "      <td>/home/ke/Desktop/all_gas/WH_5002_20180316_EDIT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID      Date                                      EndTidal_Path\n",
       "0     BR001  20170511  /home/ke/Desktop/all_gas/BR_001_20170511_EDITS...\n",
       "1     BR006  20171107  /home/ke/Desktop/all_gas/BR_006_20171107_EDITS...\n",
       "2     BR011  20190226  /home/ke/Desktop/all_gas/BR_011_20190226_edits...\n",
       "3     BR011  20171101  /home/ke/Desktop/all_gas/BR_011_20171101_EDITS...\n",
       "4     BR012  20171108  /home/ke/Desktop/all_gas/BR_012_20171108_EDITS...\n",
       "..      ...       ...                                                ...\n",
       "139  WH1707  20190401  /home/ke/Desktop/all_gas/WH_1707_20190401_edit...\n",
       "140  WH1716  20190213  /home/ke/Desktop/all_gas/WH_1716_20190213_edit...\n",
       "141  WH1721  20190813  /home/ke/Desktop/all_gas/WH_1721_20190813_EDIT...\n",
       "142  WH1726  20190702  /home/ke/Desktop/all_gas/WH_1726_20190702_EDIT...\n",
       "143  WH5002  20180316  /home/ke/Desktop/all_gas/WH_5002_20180316_EDIT...\n",
       "\n",
       "[144 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_files = [file for file in os.listdir(path) if file.upper().endswith('EDITS.TXT')]\n",
    "# txt_files.sort()\n",
    "# for file in txt_files:\n",
    "#     print(file)\n",
    "\n",
    "p_dic = {'ID' : [],\n",
    "         'Date' : [],\n",
    "         'EndTidal_Path' : []}\n",
    "\n",
    "for file in txt_files:\n",
    "    file, notes = extract_notes(file)\n",
    "    file = file.split('_')\n",
    "    ## NOTE: THIS NEEDS TO BE ALTERED WHEN CONVERTING TO AUTOMATED SCRIPT\n",
    "    if len(file) != 4:\n",
    "#         print(file)\n",
    "        if notes:\n",
    "            notes = notes[:-1]+'_'+'_'.join(file[3:-2])+')'\n",
    "        else:\n",
    "            notes = '('+'_'.join(file[3:-1])+')'\n",
    "    name = file[:2]\n",
    "    if notes:\n",
    "        name.append(notes)\n",
    "    name.append(file[2])\n",
    "    name.append(file[-1])\n",
    "    file = name\n",
    "    p_id = file[0].upper() + file[1]\n",
    "    p_dic['ID'].append(p_id)\n",
    "    p_dic['Date'].append(file[-2])\n",
    "    p_dic['EndTidal_Path'].append(path+'_'.join(file))\n",
    "\n",
    "p_df = pd.DataFrame(p_dic)\n",
    "\n",
    "# patient_BOLDS_header = [p_df.ID[i]+'_BOLD_'+p_df.Date[i] for i in range(len(p_df))]\n",
    "# pd.set_option('display.max_rows', p_df.shape[0]+1)\n",
    "p_df.sort_values('ID').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ke/Desktop/all_gas/SH004_206_20161220_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_004_20161220_edits.txt\n",
      "/home/ke/Desktop/all_gas/BR042_786_20181204_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_042_20181204_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH003_205_20161216_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_003_20161216_edits.txt\n",
      "/home/ke/Desktop/all_gas/SH037_321_20170517_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_037_20170517_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1604_267_20170310_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1604_20170310_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1664_807_20190104_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1664_20190104_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1651_550_20180501_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1651_20180501_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH062_1126_20190813_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_062_20190813_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH011_222_20170131_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_011_20170131_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1406_316_20170511_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1406_20170511_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH002_204_20161216_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_002_20161216_edits.txt\n",
      "/home/ke/Desktop/all_gas/BR040_782_20181203_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_040_20181203_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1601_281_20170331_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1601_20170331_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN008_288_20170406_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_008_20170406_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1618_459_20171219_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1618_20171219_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH058_593_20180607_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_058_20180607_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1721_1127_20190813_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1721_20190813_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR037_726_20181012_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_037_20181012_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH033_306_20170504_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_033_20170504_edits.txt\n",
      "/home/ke/Desktop/all_gas/BR034_653_20180814_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_034_20180814_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR044_828_20190129_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_044_(2)_20190129_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH001_202_20161213_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_001_20161213_edits.txt\n",
      "/home/ke/Desktop/all_gas/MEN027_258_20170303_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_027_20170303_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH009_214_20170112_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_009_20170112_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1419_611_20180629_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1419_20180629_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1690_694_20180913_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1690_20180913_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH063_1143_20190820_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_063_20190820_edits.txt\n",
      "/home/ke/Desktop/all_gas/BR022_476_20180130_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_022_20180130_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR019_461_20171220_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_019_20171220_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH002_1134_20190816_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_002_20190816_edits.txt\n",
      "/home/ke/Desktop/all_gas/BR025_503_20180314_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_025_20180314_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR011_425_20171101_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_011_20171101_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH039_323_20170518_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_039_20170518_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1621_530_20180416_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1621_20180416_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH007_209_20170104_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_007_20170104_edits.txt\n",
      "/home/ke/Desktop/all_gas/BR031_529_20180416_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_031_20180416_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR011_875_20190226_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_011_20190226_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1627_440_20171116_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1627_20171116_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1414_806_20190104_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1414_20190104_edits.txt\n",
      "/home/ke/Desktop/all_gas/BR017_455_20171214_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_017_20171214_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR049_1042_20190624_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_049_20190624_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR029_525_20180410_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_029_20180410_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH012_223_20170131_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_012_20170131_edits.txt\n",
      "/home/ke/Desktop/all_gas/BR013_900_20190315_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_013_20190315_edits.txt\n",
      "/home/ke/Desktop/all_gas/SH064_1151_20190823_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_064_20190823_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1686_721_20181010_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1686_20181010_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN014_253_20170227_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_014_20170227_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH5002_506_20180316_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_5002_20180316_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN017_249_20170224_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_017_20170224_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1705_853_20190215_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1705_20190215_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1128_905_20190319_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1128_20190319_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1242_815_20190115_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1242_20190115_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1698_738_20181025_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1698_20181025_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR027_513_20180328_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_027_20180328_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1492_242_20170222_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1492_20170222_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN038_296_20170418_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_038_20170418_EDITs.txt\n",
      "/home/ke/Desktop/all_gas/SH052_428_20171107_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_052_20171107_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1349_606_20180627_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1349_20180627_EDITs.txt\n",
      "/home/ke/Desktop/all_gas/BR018_850_20190214_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_018_20190214_edits.txt\n",
      "/home/ke/Desktop/all_gas/BR014_436_20171115_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_014_20171115_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH010_217_20170120_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_010_20170120_edits.txt\n",
      "/home/ke/Desktop/all_gas/MEN026_270_20170315_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_026_20170315_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH022_263_20170307_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_022_20170307_edits.txt\n",
      "/home/ke/Desktop/all_gas/BR043_826_20190128_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_043_20190128_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR023_475_20180130_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_023_20180130_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1647_446_20171204_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1647_20171204_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH008_212_20170110_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_008_20170110_edits.txt\n",
      "/home/ke/Desktop/all_gas/BR006_427_20171107_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_006_20171107_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR038_740_20181026_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_038_20181026_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH053_429_20171107_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_053_20171107_edits.txt\n",
      "/home/ke/Desktop/all_gas/SH056_578_20180523_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_056_20180523_edits.txt\n",
      "/home/ke/Desktop/all_gas/SH054_451_20171211_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_054_20171211_edits.txt\n",
      "/home/ke/Desktop/all_gas/SH026_299_20170501_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_026_20170501_edits.txt\n",
      "/home/ke/Desktop/all_gas/MEN020_254_20170301_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_020_20170301_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1449_269_20170314_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1449_20170314_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH060_1144_20190820_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_060_20190820_edits.txt\n",
      "/home/ke/Desktop/all_gas/SH024_276_20170329_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_024_20170329_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1242_580_20180530_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1242_20180530_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1360_319_20170515_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1360_20170515_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR001_315_20170511_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_001_20170511_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1330_572_20180518_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1330_20180518_edits.txt\n",
      "/home/ke/Desktop/all_gas/SH042_327_20170523_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_042_20170523_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1490_273_20170327_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1490_20170327_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN013_241_20170222_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_013_20170222_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH020_257_20170303_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_020_20170303_edits.txt\n",
      "/home/ke/Desktop/all_gas/MEN030_274_20170327_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_030_20170327_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN019_244_20170223_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_019_20170223_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH006_208_20161221_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_006_20161221_edits.txt\n",
      "/home/ke/Desktop/all_gas/SH003_1133_20190816_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_003_20190816_edits.txt\n",
      "/home/ke/Desktop/all_gas/MEN002_236_20170215_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_002_20170215_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1646_456_20171215_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1646_20171215_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR024_493_20180227_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_024_20180227_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR020_465_20180109_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_020_20180109_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH036_293_20170417_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_036_20170417_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1333_1240_20191105_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1333_20191105_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN021_256_20170302_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_021_20170302_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN044_566_20180515_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_044_20180515_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH031_310_20170505_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_031_20170505_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1726_1061_20190702_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1726_20190702_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1657_522_20180409_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1657_20180409_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1653_485_20180213_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1653_20180213_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1495_224_20170203_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1495_20170203_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN032_292_20170411_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_032_20170411_edits.txt\n",
      "/home/ke/Desktop/all_gas/SH015_226_20170208_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_015_20170208_edits.txt\n",
      "/home/ke/Desktop/all_gas/SH035_312_20170509_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_035_20170509_edits.txt\n",
      "/home/ke/Desktop/all_gas/SH023_272_20170316_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_023_20170316_edits.txt\n",
      "/home/ke/Desktop/all_gas/BR021_472_20180124_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_021_20180124_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN015_239_20170216_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_015_20170216_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR035_612_20180703_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_035_20180703_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1716_847_20190213_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1716_20190213_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1645_481_20180208_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1645_20180208_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH016_314_20170510_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_016_20170510_edits.txt\n",
      "/home/ke/Desktop/all_gas/SH025_277_20170329_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_025_20170329_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1701_788_20181206_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1701_20181206_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1662_595_20180608_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1662_20180608_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1660_526_20180411_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1660_20180411_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1622_444_20171130_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1622_20171130_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH030_298_20170419_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_030_20170419_edits.txt\n",
      "/home/ke/Desktop/all_gas/MEN031_295_20170418_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_031_20170418_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH059_733_20181018_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_059_20181018_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1655_603_20180626_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1655_20180626_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN050_758_20181109_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_050_20181109_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH1295_489_20180221_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1295_20180221_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR014_777_20181119_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_014_20181119_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR013_431_20171108_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_013_20171108_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN029_243_20170222_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/MEN_029_20170222_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH041_324_20170519_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_041_20170519_edits.txt\n",
      "/home/ke/Desktop/all_gas/BR044_828_20190129_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_044_20190129_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR015_448_20171206_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_015_20171206_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR012_430_20171108_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_012_20171108_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH034_300_20170501_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/SH_034_20170501_edits.txt\n",
      "/home/ke/Desktop/all_gas/WH1602_259_20170306_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/WH_1602_20170306_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR026_511_20180327_EDITS.txt\n",
      "/home/ke/Desktop/all_gas/BR_026_20180327_edits.txt\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(p_df)):\n",
    "    \n",
    "    #if bold file doesnt exist then continue\n",
    "    patient_dir = glob.glob(nifti_dir + p_df.ID[i] + '*' + p_df.Date[i])\n",
    "    if len(patient_dir) == 0 or not os.path.exists(patient_dir[0] + '/BOLD/'):\n",
    "        date = p_df.Date[i][-4:] + p_df.Date[i][:-4]\n",
    "        patient_dir = glob.glob(nifti_dir + p_df.ID[i] + '*' + date)\n",
    "        if len(patient_dir) == 0 or not os.path.exists(patient_dir[0] + '/BOLD/'):\n",
    "            continue\n",
    "        else:\n",
    "            p_df.Date[i] = date\n",
    "\n",
    "    patient_dir = patient_dir[0] # patient dir is a list of len 1, need to actual string\n",
    "    n_name = patient_dir.split('/')[-1].replace('WHITTIER', 'WH')\n",
    "    n_name, notes = extract_notes(n_name)\n",
    "#     print(n_name)\n",
    "    if notes:\n",
    "        n_name = n_name.split('_')\n",
    "        n_name.insert(-2, notes)\n",
    "        n_name = '_'.join(n_name)\n",
    "    print(path+n_name+'_EDITS.txt')\n",
    "    print(p_df['EndTidal_Path'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
